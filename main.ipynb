{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Felix\n",
    "Date: 2023-04-11 14:54:25\n",
    "LastEditors: Felix\n",
    "LastEditTime: 2023-04-11 19:39:55\n",
    "Description: Please enter description\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from model import BERTModel,Trainer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ML20MDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldataset = ML20MDataset(\"../bert4rec/data/ml-20m/\",4,0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path = \"../bert4rec/data/ml-20m/ratings.csv\"\n",
    "movies_path = \"../bert4rec/data/ml-20m/movies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3903</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>846509445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>846509384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3905</th>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>846510487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>839249881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>846510556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>34</td>\n",
       "      <td>733</td>\n",
       "      <td>5.0</td>\n",
       "      <td>853510848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>34</td>\n",
       "      <td>736</td>\n",
       "      <td>4.0</td>\n",
       "      <td>844965523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>34</td>\n",
       "      <td>761</td>\n",
       "      <td>3.0</td>\n",
       "      <td>852036542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>34</td>\n",
       "      <td>780</td>\n",
       "      <td>4.0</td>\n",
       "      <td>847124405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>34</td>\n",
       "      <td>786</td>\n",
       "      <td>4.0</td>\n",
       "      <td>847124248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  movieId  rating  timestamp\n",
       "3903      34        1     5.0  846509445\n",
       "3904      34        2     3.0  846509384\n",
       "3905      34        7     3.0  846510487\n",
       "3906      34       10     5.0  839249881\n",
       "3907      34       15     3.0  846510556\n",
       "...      ...      ...     ...        ...\n",
       "3991      34      733     5.0  853510848\n",
       "3992      34      736     4.0  844965523\n",
       "3993      34      761     3.0  852036542\n",
       "3994      34      780     4.0  847124405\n",
       "3995      34      786     4.0  847124248\n",
       "\n",
       "[93 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['userId']==34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"timestamp\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build map and inversed map from movieId to tokenId\n",
    "movies = sorted(data[\"movieId\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 : PAD\n",
    "# 1 : MASK\n",
    "movie_to_id = {k:i+2 for i,k in enumerate(movies)}\n",
    "id_to_movie = {movie_to_id[k]:k for k in movie_to_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_data = data.groupby(by='userId').agg(list)[\"movieId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_data = group_by_data.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set,validate_set = random_split(groups_data,[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, mapping, padding_id, mask_id, max_len = 128, train = False  ):\n",
    "        \"\"\" Dataset class object for ml-20m dataset\n",
    "\n",
    "        Args:\n",
    "            data (list): data\n",
    "            mapping (dict): the dictionary that map moive id to token id\n",
    "            padding_id(int): the token id of [PAD]\n",
    "            mask_id (int): the token id of [MASK]\n",
    "            max_len (int, optional): the maximum length of a sequence. Defaults to 128.\n",
    "            train (bool, optional): if this dataset is a training set. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.mapping = mapping\n",
    "        self.padding_id = padding_id\n",
    "        self.masked_id = mask_id\n",
    "        self.max_len = max_len\n",
    "        self.train = train\n",
    "        self.num_items = len(mapping)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        seq = self.data[index]\n",
    "        # depricate parts over max_len\n",
    "        if len(seq) > self.max_len:\n",
    "            seq = seq[:self.max_len]\n",
    "        # tokenize the sequence\n",
    "        seq = [self.mapping[x] for x in seq]\n",
    "        mask = [0 for _ in range(len(seq))]\n",
    "        # if it is training set, mask it\n",
    "        if self.train:\n",
    "            seq, mask = self.random_mask(seq)\n",
    "        # padding \n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = seq + [self.padding_id] * padding_len\n",
    "        mask = mask + [self.padding_id] * padding_len\n",
    "        return torch.LongTensor(seq), torch.LongTensor(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def random_mask(self, sequence):\n",
    "        \"\"\"randomly mask sequence use following strategy:\n",
    "           85% chance not to mask\n",
    "           15% chance to mask\n",
    "           when masking, 80% chance to use [MASK] to replace the token,\n",
    "           10% chance to replace it with an random token and 10% chance to make no change\n",
    "\n",
    "        Args:\n",
    "            sequence(iteratble) sequence to be masked\n",
    "\n",
    "        Return:\n",
    "            sequence(list) sequence after masking\n",
    "            mask(list) mask matrix\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        mask = []\n",
    "        for s in sequence:\n",
    "            prob = random.random()\n",
    "            # not mask\n",
    "            if prob < 0.85:\n",
    "                tokens.append(s)\n",
    "                mask.append(0)\n",
    "            # mask\n",
    "            else:\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    tokens.append(self.masked_id)\n",
    "                elif prob < 0.9:\n",
    "                    tokens.append(random.randint(2,self.num_items+1))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "                mask.append(s) \n",
    "        return tokens, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "decay_steps=25\n",
    "gamma = 0.01\n",
    "weight_decay = 0.01\n",
    "model = BERTModel(128,len(movies),2,4,256,0.1)\n",
    "optimizer = optim.Adam(model.parameters(),lr = lr, weight_decay= weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=decay_steps, gamma=gamma)\n",
    "train_dataset = BERTDataset(train_set, movie_to_id, 0, 1, 128, True)\n",
    "val_dataset = BERTDataset(train_set, movie_to_id, 0, 1, 128, False)\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=64,shuffle=False, drop_last=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTModel(128,len(movies),2,4,256,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trainer = Trainer(model,train_loader,val_loader,'./checkpoint/',device,optimizer,loss_fn,lr_scheduler,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss 7.680707 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [10:10<00:00,  2.84it/s]\n",
      "Epoch 2, loss 7.685313 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [10:04<00:00,  2.86it/s]\n",
      "Epoch 3, loss 7.680290 :   3%|â–Ž         | 50/1731 [00:17<09:43,  2.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bert_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Documents/datamining/RecModel/Transformer/utils/utils.py:191\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m     \u001b[39m# self.validate(0)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs):\n\u001b[0;32m--> 191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_step(epoch)\n\u001b[1;32m    192\u001b[0m         \u001b[39m# self.validate(epoch)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         torch\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    194\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexport_path, \u001b[39mstr\u001b[39m(epoch)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.model\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/datamining/RecModel/Transformer/utils/utils.py:147\u001b[0m, in \u001b[0;36mTrainer.train_one_step\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    145\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m    146\u001b[0m \u001b[39m#  calcuate logits\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m    148\u001b[0m logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    149\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/datamining/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/datamining/RecModel/Transformer/models/bert.py:15\u001b[0m, in \u001b[0;36mBERTModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(x)\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(x)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/datamining/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/datamining/RecModel/Transformer/models/bert_modules/bert.py:40\u001b[0m, in \u001b[0;36mBERT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 40\u001b[0m     mask \u001b[39m=\u001b[39m (x \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[39m# embedding the indexed sequence to sequence of vectors\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1164068198.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    from .\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
